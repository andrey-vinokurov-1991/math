{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-dimensional Gradient Descent\n",
    "\n",
    "To find the minimum of differentiable function $f : [a, b] \\rightarrow \\mathbb{R}$ we can use the following **One-dimensional $Gradient$ $Descent$ Algorithm **\n",
    "\n",
    "1. Choose a point  $r_1 \\in [a, b]$\n",
    "2. Define $i$ as a step number of Gradient Descent. For now it is $i = 1$\n",
    "3. Calculate $f'(r_i)$.\n",
    "4. If $f'(r_i)=0$: the algorithm stops.  \n",
    "   If $f'(r_i)>0$: we should move to left, so we choose $\\delta > 0$ and assign $r_{i+1}=r_i-\\delta$.  \n",
    "   If $f'(r_i)<0$: we should move to right, so we choose $\\delta > 0$ and assign $r_{i+1}=r_i+\\delta$.  \n",
    "5. Replace $i$ to $i+1$ and repeat the steps 3, 4 and 5.\n",
    "\n",
    "If in Gradient Descent Algorithm we take a step $-\\lambda f'(r_i)$ for some positive value $\\lambda > 0$, then this $\\lambda$ is called $learning\\ rate$. In this case in the point 4 of the algorithm $r_{i+1}=r_i-\\lambda f'(r_i)$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
